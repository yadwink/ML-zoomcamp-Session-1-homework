{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "\n",
    "Use this notebook as a starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data:\n",
    "\n",
    "- https://github.com/gastonstat/CreditScoring\n",
    "- Also available [here](https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-06-trees/CreditScoring.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-10-04 10:41:14--  https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-06-trees/CreditScoring.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 182489 (178K) [text/plain]\n",
      "Saving to: 'CreditScoring.csv.10'\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 28% 1.23M 0s\n",
      "    50K .......... .......... .......... .......... .......... 56% 38.2M 0s\n",
      "   100K .......... .......... .......... .......... .......... 84% 91.3M 0s\n",
      "   150K .......... .......... ........                        100%  130M=0.04s\n",
      "\n",
      "2021-10-04 10:41:21 (4.17 MB/s) - 'CreditScoring.csv.10' saved [182489/182489]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/alexeygrigorev/mlbookcamp-code/master/chapter-06-trees/CreditScoring.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation \n",
    "\n",
    "We'll talk about this dataset in more details in week 6. But for now, use the following code to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('CreditScoring.csv')\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the features are encoded as numbers. Use the following code to de-code them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_values = {\n",
    "    1: 'ok',\n",
    "    2: 'default',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.status = df.status.map(status_values)\n",
    "\n",
    "\n",
    "home_values = {\n",
    "    1: 'rent',\n",
    "    2: 'owner',\n",
    "    3: 'private',\n",
    "    4: 'ignore',\n",
    "    5: 'parents',\n",
    "    6: 'other',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.home = df.home.map(home_values)\n",
    "\n",
    "marital_values = {\n",
    "    1: 'single',\n",
    "    2: 'married',\n",
    "    3: 'widow',\n",
    "    4: 'separated',\n",
    "    5: 'divorced',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.marital = df.marital.map(marital_values)\n",
    "\n",
    "records_values = {\n",
    "    1: 'no',\n",
    "    2: 'yes',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.records = df.records.map(records_values)\n",
    "\n",
    "job_values = {\n",
    "    1: 'fixed',\n",
    "    2: 'partime',\n",
    "    3: 'freelance',\n",
    "    4: 'others',\n",
    "    0: 'unk'\n",
    "}\n",
    "\n",
    "df.job = df.job.map(job_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['income', 'assets', 'debt']:\n",
    "    df[c] = df[c].replace(to_replace=99999999, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove clients with unknown default status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.status != 'unk'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['default'] = (df.status == 'default').astype(int)\n",
    "del df['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the categorical variables? What are the numerical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seniority     int64\n",
       "home         object\n",
       "time          int64\n",
       "age           int64\n",
       "marital      object\n",
       "records      object\n",
       "job          object\n",
       "expenses      int64\n",
       "income        int64\n",
       "assets        int64\n",
       "debt          int64\n",
       "amount        int64\n",
       "price         int64\n",
       "default       int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the all columns\n",
    "df.columns\n",
    "# look at the data types first:\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then numerical ones are: 'seniority', 'time', 'income', 'debt', 'assets', and rest of them are categorical\n",
    "categorical = ['home', 'marital', 'records', 'job']\n",
    "numerical = ['seniority', 'time', 'age', 'expenses', 'income', 'assets', 'debt', 'amount','price', 'default' ]\n",
    "\n",
    "# another way of selecting all categorical and numerical variables from a dataset\n",
    "#categorical = list(df.select_dtypes ('object'))\n",
    "#categorical\n",
    "\n",
    "#numerical = list(df.select_dtypes ('int64'))\n",
    "#numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into 3 parts: train/validation/test with 60%/20%/20% distribution. Use `train_test_split` funciton for that with `random_state=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all libraries\n",
    "\n",
    "df_full_train, df_test = train_test_split(df, test_size = 0.2, random_state = 1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size = 0.25, random_state = 1)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "# default_payment is the target variable\n",
    "y_train = df_train.default.values\n",
    "y_val = df_val.default.values\n",
    "y_test = df_test.default.values\n",
    "\n",
    "# delete the target variable\n",
    "#del df_train['default']\n",
    "#del df_val['default']\n",
    "#del df_test['default']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "ROC AUC could also be used to evaluate feature importance of numerical variables. \n",
    "\n",
    "Let's do that\n",
    "\n",
    "* For each numerical variable, use it as score and compute AUC with the \"default\" variable\n",
    "* Use the training dataset for that\n",
    "\n",
    "\n",
    "If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "\n",
    "(e.g. `-df_train['expenses']`)\n",
    "\n",
    "AUC can go below 0.5 if the variable is negatively correlated with the target varialble. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seniority 0.7093778624491943\n",
      "age 0.5732933272499939\n",
      "expenses 0.5009184217217011\n",
      "income 0.682006666132633\n",
      "assets 0.6486042567122802\n",
      "debt 0.5047829675783548\n"
     ]
    }
   ],
   "source": [
    "# auc for all numerical variables\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "for n in numerical:\n",
    "    fpr, tpr, thresholds = roc_curve(y_train, df_train[n].values)\n",
    "    auc_score= auc(fpr, tpr)\n",
    "    #print(n,auc_score )\n",
    "    \n",
    "    if auc_score < 0.5:\n",
    "        fpr, tpr, thresholds = roc_curve(y_train, -df_train[n].values)\n",
    "        auc_score= auc(fpr, tpr)\n",
    "        print(n,auc_score )        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which numerical variable (among the following 4) has the highest AUC?\n",
    "\n",
    "- seniority\n",
    "- time\n",
    "- income\n",
    "- debt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 1: seniority has the highest AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "From now on, use these columns only:\n",
    "\n",
    "```\n",
    "['seniority', 'income', 'assets', 'records', 'job', 'home']\n",
    "```\n",
    "\n",
    "Apply one-hot-encoding using `DictVectorizer` and train the logistic regression with these parameters:\n",
    "\n",
    "```\n",
    "LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7800224466891134"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['seniority', 'income', 'assets', 'records', 'job', 'home']\n",
    "\n",
    "# first logistic regression\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "train_dict = df_train[features].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "val_dict = df_val[features].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dict)\n",
    "\n",
    "y_pred = model.predict_proba(X_val)[:, 1]\n",
    "default_decision = (y_pred >= 0.5)\n",
    "(y_val == default_decision).mean()\n",
    "\n",
    "accuracy = (y_val == default_decision).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.812"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# auc of the model above\n",
    "auc_model = roc_auc_score(y_val, y_pred)\n",
    "auc_model.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "\n",
    "- 0.512\n",
    "- 0.612\n",
    "- 0.712\n",
    "- 0.812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 2: The AUC of the model is 0.812 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Now let's compute precision and recall for our model.\n",
    "\n",
    "* Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "* For each threshold, compute precision and recall\n",
    "* Plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaur\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.linspace(0, 1, 101)\n",
    "scores =[]\n",
    "\n",
    "for t in thresholds:\n",
    "    actual_positive = (y_val == 1) # we need our confusion matrix here for all thresholds\n",
    "    actual_negative = (y_val == 0)\n",
    "    \n",
    "    predict_positive = (y_pred >= t)\n",
    "    predict_negative = (y_pred < t)\n",
    "    \n",
    "    tp = (predict_positive & actual_positive).sum()\n",
    "    tn = (predict_negative & actual_negative).sum()\n",
    "    \n",
    "    fp = (predict_positive & actual_negative).sum()\n",
    "    fn = (predict_negative & actual_positive).sum()\n",
    "    \n",
    "    p = tp/(tp+fp) #precision\n",
    "    r = tp/(tp+fn) #recall\n",
    "    \n",
    "    scores.append((t,p,r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.276094</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.279228</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.284714</td>\n",
       "      <td>0.991870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.289820</td>\n",
       "      <td>0.983740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.300621</td>\n",
       "      <td>0.983740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.304071</td>\n",
       "      <td>0.971545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.312418</td>\n",
       "      <td>0.971545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.320323</td>\n",
       "      <td>0.967480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.329640</td>\n",
       "      <td>0.967480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.339056</td>\n",
       "      <td>0.963415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.345588</td>\n",
       "      <td>0.955285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.353120</td>\n",
       "      <td>0.943089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.365660</td>\n",
       "      <td>0.934959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.379768</td>\n",
       "      <td>0.930894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.388136</td>\n",
       "      <td>0.930894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.403540</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.410681</td>\n",
       "      <td>0.906504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.418251</td>\n",
       "      <td>0.894309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.427734</td>\n",
       "      <td>0.890244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.438878</td>\n",
       "      <td>0.890244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.446058</td>\n",
       "      <td>0.873984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.455724</td>\n",
       "      <td>0.857724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.467849</td>\n",
       "      <td>0.857724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.473804</td>\n",
       "      <td>0.845528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.483412</td>\n",
       "      <td>0.829268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.491443</td>\n",
       "      <td>0.817073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.493734</td>\n",
       "      <td>0.800813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.501299</td>\n",
       "      <td>0.784553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.512064</td>\n",
       "      <td>0.776423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.522099</td>\n",
       "      <td>0.768293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.146341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.134146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.134146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.126016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.113821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.113821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.105691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.093496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.089431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.085366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.085366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.081301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.052846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.044715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.044715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.036585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.032520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.028455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.024390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.016260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.016260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     threshold  precision    recall\n",
       "0         0.00   0.276094  1.000000\n",
       "1         0.01   0.279228  1.000000\n",
       "2         0.02   0.284714  0.991870\n",
       "3         0.03   0.289820  0.983740\n",
       "4         0.04   0.300621  0.983740\n",
       "5         0.05   0.304071  0.971545\n",
       "6         0.06   0.312418  0.971545\n",
       "7         0.07   0.320323  0.967480\n",
       "8         0.08   0.329640  0.967480\n",
       "9         0.09   0.339056  0.963415\n",
       "10        0.10   0.345588  0.955285\n",
       "11        0.11   0.353120  0.943089\n",
       "12        0.12   0.365660  0.934959\n",
       "13        0.13   0.379768  0.930894\n",
       "14        0.14   0.388136  0.930894\n",
       "15        0.15   0.403540  0.926829\n",
       "16        0.16   0.410681  0.906504\n",
       "17        0.17   0.418251  0.894309\n",
       "18        0.18   0.427734  0.890244\n",
       "19        0.19   0.438878  0.890244\n",
       "20        0.20   0.446058  0.873984\n",
       "21        0.21   0.455724  0.857724\n",
       "22        0.22   0.467849  0.857724\n",
       "23        0.23   0.473804  0.845528\n",
       "24        0.24   0.483412  0.829268\n",
       "25        0.25   0.491443  0.817073\n",
       "26        0.26   0.493734  0.800813\n",
       "27        0.27   0.501299  0.784553\n",
       "28        0.28   0.512064  0.776423\n",
       "29        0.29   0.522099  0.768293\n",
       "..         ...        ...       ...\n",
       "71        0.71   0.705882  0.146341\n",
       "72        0.72   0.702128  0.134146\n",
       "73        0.73   0.702128  0.134146\n",
       "74        0.74   0.688889  0.126016\n",
       "75        0.75   0.700000  0.113821\n",
       "76        0.76   0.700000  0.113821\n",
       "77        0.77   0.742857  0.105691\n",
       "78        0.78   0.718750  0.093496\n",
       "79        0.79   0.733333  0.089431\n",
       "80        0.80   0.807692  0.085366\n",
       "81        0.81   0.807692  0.085366\n",
       "82        0.82   0.833333  0.081301\n",
       "83        0.83   0.857143  0.073171\n",
       "84        0.84   0.812500  0.052846\n",
       "85        0.85   0.785714  0.044715\n",
       "86        0.86   0.785714  0.044715\n",
       "87        0.87   0.750000  0.036585\n",
       "88        0.88   0.727273  0.032520\n",
       "89        0.89   0.777778  0.028455\n",
       "90        0.90   0.750000  0.024390\n",
       "91        0.91   0.800000  0.016260\n",
       "92        0.92   0.800000  0.016260\n",
       "93        0.93   1.000000  0.008130\n",
       "94        0.94        NaN  0.000000\n",
       "95        0.95        NaN  0.000000\n",
       "96        0.96        NaN  0.000000\n",
       "97        0.97        NaN  0.000000\n",
       "98        0.98        NaN  0.000000\n",
       "99        0.99        NaN  0.000000\n",
       "100       1.00        NaN  0.000000\n",
       "\n",
       "[101 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put them into a dataframe:\n",
    "columns = ['threshold','precision', 'recall']\n",
    "df_scores = pd.DataFrame(scores, columns = columns)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x237da5fea58>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's plot them:\n",
    "plt.plot(df_scores.threshold, df_scores['precision'], label ='Precision')\n",
    "plt.plot(df_scores.threshold, df_scores['recall'], label = 'Recall')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At which threshold precision and recall curves intersect?\n",
    "\n",
    "* 0.2\n",
    "* 0.4\n",
    "* 0.6\n",
    "* 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 3: The precision and recall curves meet at threshold 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Precision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both\n",
    "\n",
    "This is the formula for computing F1:\n",
    "\n",
    "$$F_1 = 2 \\cdot \\cfrac{P \\cdot R}{P + R}$$\n",
    "\n",
    "Where $P$ is precision and $R$ is recall.\n",
    "\n",
    "Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaur\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.linspace(0, 1, 101)\n",
    "scores =[]\n",
    "\n",
    "for t in thresholds:\n",
    "    actual_positive = (y_val == 1) # we need our confusion matrix here for all thresholds\n",
    "    actual_negative = (y_val == 0)\n",
    "    \n",
    "    predict_positive = (y_pred >= t)\n",
    "    predict_negative = (y_pred < t)\n",
    "    \n",
    "    tp = (predict_positive & actual_positive).sum()\n",
    "    tn = (predict_negative & actual_negative).sum()\n",
    "    \n",
    "    fp = (predict_positive & actual_negative).sum()\n",
    "    fn = (predict_negative & actual_positive).sum()\n",
    "    \n",
    "    p = tp/(tp+fp) #precision\n",
    "    r = tp/(tp+fn) #recall\n",
    "    f1 = 2*p*r/(p+r)\n",
    "    \n",
    "    scores.append((t,p,r,f1))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.276094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.432718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.279228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.436557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.284714</td>\n",
       "      <td>0.991870</td>\n",
       "      <td>0.442430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.289820</td>\n",
       "      <td>0.983740</td>\n",
       "      <td>0.447734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.300621</td>\n",
       "      <td>0.983740</td>\n",
       "      <td>0.460514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.304071</td>\n",
       "      <td>0.971545</td>\n",
       "      <td>0.463178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.312418</td>\n",
       "      <td>0.971545</td>\n",
       "      <td>0.472799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.320323</td>\n",
       "      <td>0.967480</td>\n",
       "      <td>0.481294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.329640</td>\n",
       "      <td>0.967480</td>\n",
       "      <td>0.491736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.339056</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.501587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.345588</td>\n",
       "      <td>0.955285</td>\n",
       "      <td>0.507559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.353120</td>\n",
       "      <td>0.943089</td>\n",
       "      <td>0.513843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.365660</td>\n",
       "      <td>0.934959</td>\n",
       "      <td>0.525714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.379768</td>\n",
       "      <td>0.930894</td>\n",
       "      <td>0.539458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.14</td>\n",
       "      <td>0.388136</td>\n",
       "      <td>0.930894</td>\n",
       "      <td>0.547847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.403540</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.562269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.410681</td>\n",
       "      <td>0.906504</td>\n",
       "      <td>0.565272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.418251</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>0.569948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.427734</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.577836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.438878</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.587919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.446058</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0.590659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.455724</td>\n",
       "      <td>0.857724</td>\n",
       "      <td>0.595205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.467849</td>\n",
       "      <td>0.857724</td>\n",
       "      <td>0.605452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.473804</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.607299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.483412</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.610778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.491443</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.613740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.493734</td>\n",
       "      <td>0.800813</td>\n",
       "      <td>0.610853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.27</td>\n",
       "      <td>0.501299</td>\n",
       "      <td>0.784553</td>\n",
       "      <td>0.611727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.512064</td>\n",
       "      <td>0.776423</td>\n",
       "      <td>0.617124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.522099</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.621711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.242424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>0.225256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>0.225256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.126016</td>\n",
       "      <td>0.213058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.195804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.195804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.185053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.093496</td>\n",
       "      <td>0.165468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.159420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.085366</td>\n",
       "      <td>0.154412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.085366</td>\n",
       "      <td>0.154412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.081301</td>\n",
       "      <td>0.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.134831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.052846</td>\n",
       "      <td>0.099237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.044715</td>\n",
       "      <td>0.084615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.044715</td>\n",
       "      <td>0.084615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.069767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.062257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.028455</td>\n",
       "      <td>0.054902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.047244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.031873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.031873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.016129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     threshold  precision    recall  f1_scores\n",
       "0         0.00   0.276094  1.000000   0.432718\n",
       "1         0.01   0.279228  1.000000   0.436557\n",
       "2         0.02   0.284714  0.991870   0.442430\n",
       "3         0.03   0.289820  0.983740   0.447734\n",
       "4         0.04   0.300621  0.983740   0.460514\n",
       "5         0.05   0.304071  0.971545   0.463178\n",
       "6         0.06   0.312418  0.971545   0.472799\n",
       "7         0.07   0.320323  0.967480   0.481294\n",
       "8         0.08   0.329640  0.967480   0.491736\n",
       "9         0.09   0.339056  0.963415   0.501587\n",
       "10        0.10   0.345588  0.955285   0.507559\n",
       "11        0.11   0.353120  0.943089   0.513843\n",
       "12        0.12   0.365660  0.934959   0.525714\n",
       "13        0.13   0.379768  0.930894   0.539458\n",
       "14        0.14   0.388136  0.930894   0.547847\n",
       "15        0.15   0.403540  0.926829   0.562269\n",
       "16        0.16   0.410681  0.906504   0.565272\n",
       "17        0.17   0.418251  0.894309   0.569948\n",
       "18        0.18   0.427734  0.890244   0.577836\n",
       "19        0.19   0.438878  0.890244   0.587919\n",
       "20        0.20   0.446058  0.873984   0.590659\n",
       "21        0.21   0.455724  0.857724   0.595205\n",
       "22        0.22   0.467849  0.857724   0.605452\n",
       "23        0.23   0.473804  0.845528   0.607299\n",
       "24        0.24   0.483412  0.829268   0.610778\n",
       "25        0.25   0.491443  0.817073   0.613740\n",
       "26        0.26   0.493734  0.800813   0.610853\n",
       "27        0.27   0.501299  0.784553   0.611727\n",
       "28        0.28   0.512064  0.776423   0.617124\n",
       "29        0.29   0.522099  0.768293   0.621711\n",
       "..         ...        ...       ...        ...\n",
       "71        0.71   0.705882  0.146341   0.242424\n",
       "72        0.72   0.702128  0.134146   0.225256\n",
       "73        0.73   0.702128  0.134146   0.225256\n",
       "74        0.74   0.688889  0.126016   0.213058\n",
       "75        0.75   0.700000  0.113821   0.195804\n",
       "76        0.76   0.700000  0.113821   0.195804\n",
       "77        0.77   0.742857  0.105691   0.185053\n",
       "78        0.78   0.718750  0.093496   0.165468\n",
       "79        0.79   0.733333  0.089431   0.159420\n",
       "80        0.80   0.807692  0.085366   0.154412\n",
       "81        0.81   0.807692  0.085366   0.154412\n",
       "82        0.82   0.833333  0.081301   0.148148\n",
       "83        0.83   0.857143  0.073171   0.134831\n",
       "84        0.84   0.812500  0.052846   0.099237\n",
       "85        0.85   0.785714  0.044715   0.084615\n",
       "86        0.86   0.785714  0.044715   0.084615\n",
       "87        0.87   0.750000  0.036585   0.069767\n",
       "88        0.88   0.727273  0.032520   0.062257\n",
       "89        0.89   0.777778  0.028455   0.054902\n",
       "90        0.90   0.750000  0.024390   0.047244\n",
       "91        0.91   0.800000  0.016260   0.031873\n",
       "92        0.92   0.800000  0.016260   0.031873\n",
       "93        0.93   1.000000  0.008130   0.016129\n",
       "94        0.94        NaN  0.000000        NaN\n",
       "95        0.95        NaN  0.000000        NaN\n",
       "96        0.96        NaN  0.000000        NaN\n",
       "97        0.97        NaN  0.000000        NaN\n",
       "98        0.98        NaN  0.000000        NaN\n",
       "99        0.99        NaN  0.000000        NaN\n",
       "100       1.00        NaN  0.000000        NaN\n",
       "\n",
       "[101 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put them into a dataframe:\n",
    "columns = ['threshold','precision', 'recall','f1_scores']\n",
    "df_scores = pd.DataFrame(scores, columns = columns)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.535817</td>\n",
       "      <td>0.760163</td>\n",
       "      <td>0.628571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold  precision    recall  f1_scores\n",
       "30        0.3   0.535817  0.760163   0.628571"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores[df_scores['f1_scores'] == df_scores.f1_scores.max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At which threshold F1 is maximal?\n",
    "\n",
    "- 0.1\n",
    "- 0.3\n",
    "- 0.5\n",
    "- 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 4: at threshold 0.3, the F1 is maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "\n",
    "Use the `KFold` class from Scikit-Learn to evaluate our model on 5 different folds:\n",
    "\n",
    "```\n",
    "KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "```\n",
    "\n",
    "* Iterate over different folds of `df_full_train`\n",
    "* Split the data into train and validation\n",
    "* Train the model on train with these parameters: `LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)`\n",
    "* Use AUC to evaluate the model on validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_train, y_train, c=1.0):\n",
    "    dicts = df_train[features].to_dict(orient='records')\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    X_train = dv.fit_transform(dicts)\n",
    "    \n",
    "    model = LogisticRegression(solver='liblinear', C=c, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return dv, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, dv, model):\n",
    "    dicts = df[features].to_dict(orient='records')\n",
    "    \n",
    "    X = dv.transform(dicts)\n",
    "    y_pred = model.predict_proba(X)[:,1]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits = 5, shuffle=True, random_state=1) # splitting our data into 5 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814 +- 0.015\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(df_full_train):\n",
    "    df_train = df_full_train.iloc[train_idx]\n",
    "    df_val = df_full_train.iloc[val_idx]\n",
    "    \n",
    "    y_train = df_train.default.values\n",
    "    y_val = df_val.default.values\n",
    "    \n",
    "    dv, model = train(df_train, y_train)\n",
    "    y_pred = predict(df_val, dv, model)\n",
    "    \n",
    "    auc = roc_auc_score(y_val, y_pred)\n",
    "    scores.append(auc)\n",
    "        \n",
    "print('%.3f +- %.3f' % (np.mean(scores), np.std(scores)) )    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How large is standard devidation of the scores across different folds?\n",
    "\n",
    "- 0.001\n",
    "- 0.014\n",
    "- 0.09\n",
    "- 0.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 5: The standard deviation is 0.014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Now let's use 5-Fold cross-validation to find the best parameter C\n",
    "\n",
    "* Iterate over the following C values: `[0.01, 0.1, 1, 10]`\n",
    "* Initialize `KFold` with the same parameters as previously\n",
    "* Use these parametes for the model: `LogisticRegression(solver='liblinear', C=C, max_iter=1000)`\n",
    "* Compute the mean score as well as the std (round the mean and std to 3 decimal digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d849451061f641059fe03967d8e33e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01 0.808 +- 0.012\n",
      "C=0.1 0.813 +- 0.014\n",
      "C=1 0.814 +- 0.015\n",
      "C=5 0.814 +- 0.015\n",
      "C=10 0.814 +- 0.015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now we will proceed with our usual procedure:\n",
    "# extract the dictionary, feed the dictionary vectorizer, extract the feature matrix, train a model\n",
    "# iterate for different values of c\n",
    "from tqdm.auto import tqdm\n",
    "n_splits = 5\n",
    "\n",
    "for C in tqdm([0.01,0.1,1,5,10]):\n",
    "\n",
    "    scores = []\n",
    "    kfold = KFold(n_splits = n_splits, shuffle=True, random_state=1) \n",
    "\n",
    "    for train_idx, val_idx in kfold.split(df_full_train):\n",
    "        df_train = df_full_train.iloc[train_idx]\n",
    "        df_val = df_full_train.iloc[val_idx]\n",
    "    \n",
    "        y_train = df_train.default.values\n",
    "        y_val = df_val.default.values\n",
    "    \n",
    "        dv, model = train(df_train, y_train, c=C)\n",
    "        y_pred = predict(df_val, dv, model)\n",
    "    \n",
    "        auc = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(auc)\n",
    "        \n",
    "    print('C=%s %.3f +- %.3f' % (C, np.mean(scores), np.std(scores)) )\n",
    "# we see that for smaller values of C, auc dropped    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which C leads to the best mean score?\n",
    "\n",
    "- 0.01\n",
    "- 0.1\n",
    "- 1\n",
    "- 10\n",
    "\n",
    "If you have ties, select the score with the lowest std. If you still have ties, select the smallest C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer 6: The value of C = 1 leads to the smallest std deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "Submit your results here: https://forms.gle/e497sR5iB36mM9Cs5\n",
    "\n",
    "It's possible that your answers won't match exactly. If it's the case, select the closest one.\n",
    "\n",
    "## Deadline\n",
    "\n",
    "The deadline for submitting is 04 October 2021, 17:00 CET. After that, the form will be closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
